HOSPITAL IS UNZIPPED TRANSFER IT!!!! AND ALSO OFFICE 2!!!!

STOPPED AT 67K iterations.
make sure all the lost above is completed before continuing!

--- CHECK IF RELU IN THE END ON THE SUMMATION KILLS THE GRADIENT AND THEN NEAURONS DOESNT GET UPDATED!

0) check online how to test if my layers learn or not by their distribution, how to see if gradient is sufficient!!!
1) read the article, make sure everything in place and all the EPE and loss are understood, why concat and the whole logic behind the architecture.
2) coarse is pretty uniform, why?
3) refine is noisy, why?
4) maybe tensorboard coarse and refine alone (and not the summation? - why concating and convolving should work? why does convolving them help the disparity inference...)
5) add to tensor board the refined and the coarse and only then the summation...



make sure all dataset disparity below 144!!!
ADD EVALUATION LOSS TO TENESORBOARD
ADD important layers weight to tensorboard
BETTER UNDERSTAND EPE AND AVG EPE AND MAKE SURE DISP MAPS SEEN WELL!!!!!!
UNDERSTAND WHY CONCATING IS GOOD IN COST VOLUME
Start preparing shifted structured light images to make inside 144 disp max!

- BATCH NORM IS DISABLED HERE, maybe add batch norm on some of the conv blocks before the leakey relu
- add dropout? 



SGD EXPLAIN:
Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties 
(e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the
 actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).
 Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in trade for a lower convergence rate.[1]

While the basic idea behind stochastic approximation can be traced back to the Robbinsâ€“Monro algorithm of the 1950s,
 stochastic gradient descent has become an important optimization method in machine learning.[2]
 
 
 
VANISHING EXPLODING GRADIENTS
with tf.name_scope('train'): 
optimizer = tf.train.AdamOptimizer() 
# Get the gradient pairs (Tensor, Variable) 
grads = optimizer.compute_gradients(cross_entropy) 
# Update the weights wrt to the gradient 
train_step = optimizer.apply_gradients(grads) 
# Save the grads with tf.summary.histogram 
for index, grad in enumerate(grads): 
    tf.summary.histogram("{}-grad".format(grads[index][1].name), grads[index]) 
Just in case you would like to understand the vanishing / exploding gradients from the values, you can simply follow the logic as below:

If you face with exploding gradients, it will increase some of the weights dramatically high, which will eventually hit to NaN and will make loss NaN. Hence, that would be a typical output of an exploding gradient.
If you face with vanishing gradient, you shall observe that the weights of all or some of the layers to be completely same over few iteration / epoch. Please note that you cannot really set a rule as "%X percent to
 detect vanishing gradients", as the loss is based on the momentum and learning rate. Learning rate and/or momentum may be low enough to cause vanishing gradients for a while,
 then if any of them gets high enough, it may break the vanishing gradient problem. What you aim in a proper training is smoothly changing histograms over iterations, rather than constant 
 weights and bias distributions over time.